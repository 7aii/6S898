---
layout: distill
title: Grokking
description: What sparks the mysterious phenomenon of “grokking”? Our blog is set to investigate this enigma. We aim to explore the various triggers and theories behind grokking to gain a deeper understanding of why these moments of unexpected intelligence arise.
date: 2023-12-12
htmlwidgets: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Tai Fuangkawinsombut
    affiliations:
      name: MEng 6-3, MIT 
  - name: Thana Somsirivattana
    affiliations:
      name: BS 18 & 6-3, MIT

# must be the exact same name as your blogpost
bibliography: 2023-12-12-grokking.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Introduction
  - name: Methodology
    subsections:
    - name: Hypothesis 1
    - name: Hypothesis 2

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction

“Grokking” is a phenomenon first observed by Power et al. (2022)<d-cite key="power2022grokking"></d-cite> in which a model trained on small algorithmically generated datasets suddenly generalizes long after fitting the training data. Since the initial work of Power et al. (2022)<d-cite key="power2022grokking"></d-cite>, various theories have been proposed to explain why grokking occurs. Some notable theories include the one proposed by Liu et al. (2023)<d-cite key="liu2023omnigrok"></d-cite> which focuses on the scale of parameters at initialization, and the one proposed by Thilak et al. (2022)<d-cite key="thilak2022slingshot"></d-cite> which focuses on the behavior of adaptive optimizers. Another theory, which can be traced back to Barak et al. (2023)<d-cite key="barak2023hidden"></d-cite> and which has attracted considerable followers, explains grokking through the slow emergence of good representations. We call the explanation along this line the “learned-representation explanation.” 

Our exploration of the grokking phenomenon supports this learned-representation explanation. As such, we review some of the literature which proposes various extensions to this explanation. Liu et al. (2022)<d-cite key="liu2022towards"></d-cite> explains grokking as a phrase between “comprehension” and “memorization” when the hyperparameters vary. Davies et al. (2023)<d-cite key="davies2023unifying"></d-cite> claims that there is an inductive bias in favor of a poorly-generalizing pattern and against a well-generalizing one. Merrill et al. (2023)<d-cite key="merrill2023tale"></d-cite> distinguishes the two patterns through their sparsity and views the training process as a competition between the sparse and dense subnetworks. Recently, Varma et al. (2023)<d-cite key="varma2023explaining"></d-cite> identifies a mechanism by which a memorizing solution is preferred over a generalizing one. We will revisit this mechanism below.

Although it is not in the scope of our project, we note that grokking has been observed empirically in other settings and model architectures beyond the ones used by Power et al. (2022)<d-cite key="power2022grokking"></d-cite>. Notably, Liu et al. (2022)<d-cite key="liu2022towards"></d-cite> reports that grokking occurs in various natural machine learning tasks such as image classification and sentiment analysis. Understanding why grokking occurs, we hope, will lead to more principled applications of machine learning models to these tasks.

## Methodology

Rather than exploring grokking in sophisticated machine learning tasks, we choose to return to the algorithmically generated datasets in the spirit of Power et al. (2022)<d-cite key="power2022grokking"></d-cite>. In particular, we consider the $$(n,k)$$-parity learning task studied by Barak et al. (2023)<d-cite key="barak2023hidden"></d-cite> and Merrill et al. (2023)<d-cite key="merrill2023tale"></d-cite>. It is defined as follows. The training and testing data consist of randomly sampled vector $$x\in \{ -1,1\}^n$$. The label of a sample $$x$$ is determined by the $$(n,k)$$-parity function

$$
f(x)=\prod_{i=1}^k x_i.
$$

We set $$n=50$$ and $$k=2$$, and fix the number of training and testing samples to be $$300$$ and $$100$$, respectively. As for the model architecture, we use the simple 1-layer RELU network with $$W=500$$ neurons. This choice of dataset and model architecture is even simpler than the one used by Power et al. (2022)<d-cite key="power2022grokking"></d-cite>. It is our hope that studying this simple setting in detail will yield insights into why grokking occurs. 
We begin by reproducing the grokking phenomenon, which can be observed in the following plot of loss and accuracy over the epochs. While training the model, we use the learning rate $$\ell = 0.1$$ and weight decay $$w=0.01$$.

{% include figure.html path="assets/img/2023-12-12-grokking/fig1.png" class="img-fluid" %}
<div class="caption">
  Figure 1: Loss and accuracy, averaged over five runs.
</div>

In what follows, we say that the model has “memorized” (resp. “generalized”) when the training (resp. testing) accuracy has reached **1**. 

The main hypothesis underlying the learning-representation explanation of grokking is that there are two distinct circuits, called the memorizing circuit and the generalizing circuit, which are learned at different speeds. The $$(n,k)$$-parity learning task that we choose for our setting guarantees the existence of the generalizing circuit. Indeed, as proved in Appendix D of Merrill et al. (2023)<d-cite key="merrill2023tale"></d-cite>, there exists a 1-layer RELU network with $$2^k$$ neurons that computes the $$(n,k)$$-parity function. However, this guarantee is only theoretical, since it may happen that the circuit is not learned through the training process.
Empirically, we show the existence of both the memorizing circuit and the generalizing circuit through the following series of visualizations. 

First, we show the t-SNE plots of the learned representations at various epochs ranging from when the model has memorized to when the model has generalized:

{% include figure.html path="assets/img/2023-12-12-grokking/fig2.png" class="img-fluid" %}
<div class="caption">
  Figure 2: t-SNE plots of the learned representations at various epochs.
</div>

Here, we color the data according to the first $$k=2$$ coordinates of the sample, so there are $$2^k=4$$ total colors. The plots show a clear emergence of “good” representations after the model had already achieved memorization with “bad” representations. Indeed, the good representations can be divided into $$2^k$$ clusters depending solely on the values of the first $$k$$ coordinates, which are all that is needed to compute the $$(n,k)$$-parity function. Meanwhile, there is no discernible clustering of the bad representations.

To further distinguish the two circuits (i.e., not just representations) when the model achieves memorization from when it achieves generalization, we show the kernel density estimation (KDE) plots of the weights in the hidden layer. This time, we show the plots at epochs after the model has already achieved memorization as well.

{% include figure.html path="assets/img/2023-12-12-grokking/fig3.png" class="img-fluid" %}
<div class="caption">
  Figure 3: KDE plots of the weights at various epochs.
</div>

The plots show that, as the training process progresses, the weights coalesce around zero; in other words, the model becomes sparser. Although the difference between the plots when the model achieves memorization and when it achieves generalization is not much, it is clear that the final memorizing circuit is very sparse, which is consistent with the earlier theoretical guarantee.

One quantitative way of tracking the change of the weights is through what Barak et al. (2023)<d-cite key="barak2023hidden"></d-cite> called progress measure. Here, we choose as our progress measure the spectral norm of the weight matrix, for reasons that will become clear below. Figure 4 shows the plot of the spectral norm over the epochs.

{% include figure.html path="assets/img/2023-12-12-grokking/fig4.png" class="img-fluid" %}
<div class="caption">
  Figure 4: Spectral norm of the weight matrix, averaged over five runs.
</div>

Comparing Figure 4 with the accuracy plot in Figure 1, we notice that the spectral norm has two inflection points at roughly the same epoch when the model achieves memorization and when it achieves generalization. Between these two points, the spectral norm sharply declines.

Having seen the visualizations that show the existence of the memorizing circuit and the generalizing circuit, we must next examine how the former transitions to the latter in the training process. Without a clear understanding of this transition, the learned-representation explanation does not truly explain the grokking phenomenon. Our observations from Figures 3 and 4 prompt us to look for a mechanism that sparsifies the circuit after the model has achieved memorization. Power et al. (2022)<d-cite key="power2022grokking"></d-cite> observes that weight decay is particularly effective at inducing grokking. Combining this observation with the common wisdom that weight decay penalizes large weights in the circuit, we arrive at the suggestion in Varma et al. (2023)<d-cite key="varma2023explaining"></d-cite> that weight decay is one important ingredient in the mechanism. We formulate the following hypothesis on the effect of weight decay.

### Hypothesis 1

**If we increase the weight decay $$w$$ in the optimizer after the model has achieved memorization, then the model should achieve generalization faster. However, there should be some threshold of weight decay beyond which the model cannot achieve generalization at all.**

To us, Hypothesis 1 seems like the most natural way to test the suggestion in Varma et al. (2023)<d-cite key="varma2023explaining"></d-cite>. However, Varma et al. (2023)<d-cite key="varma2023explaining"></d-cite> primarily focuses on grokking at the “critical” dataset size and does not undertake the investigation in the form of Hypothesis 1 above.

Another method of testing the learned-representation explanation of grokking comes from an experimental setup in Power et al. (2022)<d-cite key="power2022grokking"></d-cite>, which is as follows. We “plant” some outliers in the training data, but not in the testing data. The question is whether the model would arrive at a generalizing circuit that learns to “correct” the outliers in the training data. In our setting, we introduce the parameter $$p_t\in [0,1]$$ for the probability that we “trick” the model by flipping the label of each training sample. Since the outliers should prevent the formation of the generalizing circuit more than the formation of the memorizing circuit, we formulate the following hypothesis on the effect of the fraction of outliers.

### Hypothesis 2

**If we increase the fraction of outliers $$p_t$$, then the model should achieve generalization slower. However, there should be some threshold of the fraction beyond which the model cannot achieve generalization at all.**

Although Hypothesis 2 is motivated by an experimental setup in Power et. al. (2022)<d-cite key="power2022grokking"></d-cite>, the paper only reports the result in terms of the fraction of training data needed for grokking to occur. Here, we are interested in the speed at which the generalizing circuit is learned. 
